# kaggle_Titanic
Student Project in SPICED Academy Berlin, week 02.

![GitHub Logo](/images/titanic.png "Photo Credit: @pixel2013|pixabay.com")

This is my attempt on the the legendary [kaggle](https://www.kaggle.com) competition ["Titanic: Machine Learning from Disaster"](https://www.kaggle.com/c/titanic)


## The goals of the project:
1. Exploring Titanic Dataset.
2. Create/engineer features using different sklearn transformers such as OneHotEncoder, SimpleImputer, MinMaxScaler wrapped in ColumnTransformer and/or pipeline
3. Train Logistic Regression, Decision Tree and Random Forest Classification Models.
4. Adjust hyperparameters using Cross-Validation and/or Bootstrap.
5. Uppload the best result on kaggle.com.

In this repository project is devided into 4 Jupiter Notebooks:
1. Exploratory Data Analysis
2. Feature Engineering, Training and Validating for Logistic Regression Classification Model
3. Feature Engineering, Training and Validating for Decision Tree Model
4. Feature Engineering, Training and Validating for Random Forest Classification Model

## Kaggle Results:
Logistic Regression: 0.77511

RandomForest: 0.74401

Decision Tree: 0.72248

